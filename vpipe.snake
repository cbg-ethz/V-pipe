import csv
import collections
import configparser
import os
import sys
import typing

# 1. parse config file
#
# the config object sets defaults for a number of parameters and
# validates parameters given by the user to be sensible

VPIPE_DEBUG = True if os.environ.get('VPIPE_DEBUG') is not None else False
VPIPE_BENCH = True if os.environ.get('VPIPE_BENCH') is not None else False


class VpipeConfig(object):
    'Class used to encapsulate the configuration properties used by V-pipe'

    __RECORD__ = typing.NamedTuple(
        "__RECORD__", [('value', typing.Any), ('type', type)])
    __MEMBER_DEFAULT__ = collections.OrderedDict([
        ('general', {
            'threads': __RECORD__(value=4, type=int),
            'aligner': __RECORD__(value='ngshmmalign', type=str),
            'snv_caller': __RECORD__(value='shorah', type=str),
            'haplotype_reconstruction': __RECORD__(value='savage', type=str)
        }),
        ('input', {
            'datadir': __RECORD__(value='samples', type=str),
            'samples_file': __RECORD__(value='samples.tsv', type=str),
            'paired': __RECORD__(value=True, type=bool),
            'fastq_suffix': __RECORD__(value='', type=str),
            'trim_percent_cutoff': __RECORD__(value=0.8, type=float),
            'reference': __RECORD__(value='references/HXB2.fasta', type=str),
        }),
        ('output', {
            'QA': __RECORD__(value=False, type=bool),
            'snv': __RECORD__(value=True, type=bool),
            'local': __RECORD__(value=True, type=bool),
            'global': __RECORD__(value=True, type=bool),
        }),
        ('applications', {
            'gunzip': __RECORD__(value="gunzip", type=str),
            'prinseq': __RECORD__(value="prinseq-lite.pl", type=str),
            'vicuna': __RECORD__(value="vicuna", type=str),
            'indelfixer': __RECORD__(value="IndelFixer.jar", type=str),
            'consensusfixer': __RECORD__(value="ConsensusFixer.jar", type=str),
            'picard': __RECORD__(value="picard", type=str),
            'bwa': __RECORD__(value="bwa", type=str),
            'bowtie_idx': __RECORD__(value="bowtie2-build", type=str),
            'bowtie': __RECORD__(value="bowtie2", type=str),
            'samtools': __RECORD__(value="samtools", type=str),
            'extract_consensus': __RECORD__(value="extract_consensus", type=str),
            'mafft': __RECORD__(value="mafft", type=str),
            'ngshmmalign': __RECORD__(value="ngshmmalign", type=str),
            'convert_reference': __RECORD__(value="convert_reference", type=str),
            'extract_seq': __RECORD__(value="extract_seq", type=str),
            'coverage_stats': __RECORD__(value="coverage_stats", type=str),
            'remove_gaps_msa': __RECORD__(value="remove_gaps_msa", type=str),
            'minority_freq': __RECORD__(value="minority_freq", type=str),
            'extract_coverage_intervals': __RECORD__(value="extract_coverage_intervals", type=str),
            'shorah': __RECORD__(value="shorah shotgun", type='str'),
            'lofreq': __RECORD__(value="lofreq", type=str),
            'bcftools': __RECORD__(value="bcftools", type=str),
            'haploclique': __RECORD__(value="haploclique", type='str'),
            'compute_mds': __RECORD__(value="compute_mds", type='str'),
            'savage': __RECORD__(value="savage", type='str')
        }),

        ('gunzip', {
            'mem': __RECORD__(value=30000, type=int),
            'time': __RECORD__(value=60, type=int),
        }),
        ('extract', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=20, type=int),
        }),
        ('preprocessing', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='', type=str),

            'qual_threshold': __RECORD__(value=30, type=int),
            'min_len': __RECORD__(value=0.8, type=float),
        }),
        ('initial_vicuna', {
            'mem': __RECORD__(value=1000, type=int),
            'time': __RECORD__(value=600, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('initial_vicuna_msa', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('create_vicuna_initial', {
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('hmm_align', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=1435, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),

            'leave_msa_temp': __RECORD__(value=False, type=bool),
        }),
        ('sam2bam', {
            'mem': __RECORD__(value=5000, type=int),
            'time': __RECORD__(value=30, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('bwa_QA', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('coverage_QA', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('msa', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('convert_to_ref', {
            'mem': __RECORD__(value=8000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('ref_bwa_index', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/bwa_align.yaml', type=str),
        }),
        ('bwa_align', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('ref_bowtie_index', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/bowtie_align.yaml', type=str),
        }),
        ('bowtie_align', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),

            'phred': __RECORD__(value='--phred33', type=str),
            'preset': __RECORD__(value='--local --sensitive-local', type=str),
            'report': __RECORD__(value='', type=str),
        }),
        ('consensus_sequences', {
            'mem': __RECORD__(value=1250, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),

            'min_coverage': __RECORD__(value=50, type=int),
            'qual_thrd': __RECORD__(value=15, type=int),
            'min_freq': __RECORD__(value=0.05, type=float),
        }),
        ('minor_variants', {
            'mem': __RECORD__(value=1000, type=int),
            'time': __RECORD__(value=235, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),
        }),
        ('coverage_intervals', {
            'mem': __RECORD__(value=1000, type=int),
            'time': __RECORD__(value=60, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),

            'overlap': __RECORD__(value=False, type=bool),
            'coverage': __RECORD__(value=50, type=int),
            'liberal': __RECORD__(value=True, type=bool),
        }),
        ('snv', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=2880, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),

            'shift': __RECORD__(value=3, type=int),
            'keep_files': __RECORD__(value=False, type=bool),
        }),
        ('lofreq', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=60, type=int),
            'conda': __RECORD__(value='', type=str),
        }),
        ('aggregate', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
        }),
        ('haploclique', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=1435, type=int),
            'conda': __RECORD__(value='', type=str),

            'relax': __RECORD__(value=True, type=bool),
            'no_singletons': __RECORD__(value=True, type=bool),
            'no_prob0': __RECORD__(value=True, type=bool),
            'clique_size_limit': __RECORD__(value=3, type=int),
            'max_num_cliques': __RECORD__(value=10000, type=int),
        }),
        ('haploclique_visualization', {
            'mem': __RECORD__(value=2000, type=int),
            'time': __RECORD__(value=235, type=int),
            'conda': __RECORD__(value='envs/smallgenomeutilities.yaml', type=str),

            'region_start': __RECORD__(value=0, type=int),
            'region_end': __RECORD__(value=9719, type=int),
            'msa': __RECORD__(value='', type=str),
        }),
        ('savage', {
            'mem': __RECORD__(value=10000, type=int),
            'time': __RECORD__(value=1435, type=int),
            'threads': __RECORD__(value=0, type=int),
            'conda': __RECORD__(value='', type=str),

            'split': __RECORD__(value=20, type=int),
        })
    ])

    def __init__(self):
        self.__members = {}

        vpipe_configfile = configparser.ConfigParser()
        vpipe_configfile.read('vpipe.config')

        for (section, properties) in VpipeConfig.__MEMBER_DEFAULT__.items():
            self.__members[section] = {}

            for (value, defaults) in properties.items():
                try:
                    if defaults.type == int:
                        cur_value = vpipe_configfile.getint(section, value)
                    elif defaults.type == float:
                        cur_value = vpipe_configfile.getfloat(section, value)
                    elif defaults.type == bool:
                        cur_value = vpipe_configfile.getboolean(section, value)
                    else:
                        cur_value = vpipe_configfile.get(section, value)
                    state = 'user'
                except (configparser.NoSectionError, configparser.NoOptionError):
                    if value == 'threads' and section != 'general':
                        cur_value = defaults.value if defaults.value else self.__members[
                            'general']['threads']
                    elif value == 'conda':
                        cur_value = "envs/{}.yaml".format(section) if len(
                            defaults.value) == 0 else defaults.value
                    else:
                        cur_value = defaults.value
                    state = 'DEFAULT'
                except ValueError:
                    print("ERROR: Property '{}' of section '{}' has to be of type '{}', whereas you gave '{}'!".format(
                        value, section, defaults.type.__name__, vpipe_configfile[section][value]))
                    raise

                if VPIPE_DEBUG:
                    print("Using {} value '{}' for property '{}' in section '{}'".format(
                        state, cur_value, value, section))

                self.__members[section][value] = cur_value

    def __getattr__(self, name):
        try:
            return self.__members[name]
        except:
            print("ERROR: Section '{}' is not a valid section!".format(name))
            raise


config = VpipeConfig()


# 2. glob patients/samples + store as TSV if file is not provided

# if file containing samples exists, proceed
# to build list of target files
if not os.path.isfile(config.input['samples_file']):
    # sample file does not exist, have to first glob
    # all patients' data and then construct sample
    # list that would pass QA checks

    patient_sample_pairs = glob_wildcards(
        "{}/{{patient_date}}/raw_data/{{file}}".format(config.input['datadir']))

    with open('samples.tsv', 'w') as outfile:
        for i in set(patient_sample_pairs.patient_date):
            (patient, date) = [x.strip() for x in i.split("/") if x.strip()]
            outfile.write('{}\t{}\n'.format(patient, date))

    # TODO: have to preprocess patient files to filter likely failures
    # 1.) Determine 5%/95% length of FASTQ files
    # 2.) Determine whether FASTQ would survive


# 3. load patients from TSV and create list of samples
#
# This list is reused on subsequent runs

patient_list = []
patient_dict = {}
patient_record = typing.NamedTuple(
    "patient_record", [('patient_id', str), ('date', str)])

with open(config.input['samples_file'], newline='') as csvfile:
    spamreader = csv.reader(csvfile, delimiter='\t')

    for row in spamreader:
        assert len(row) >= 2, "ERROR: Line '{}' does not contain at least two entries!".format(
                              spamreader.line_num)
        patient_tuple = patient_record(patient_id=row[0], date=row[1])
        patient_list.append(patient_tuple)

        assert config.input['trim_percent_cutoff'] > 0 and config.input['trim_percent_cutoff'] < 1, "ERROR: 'trim_percent_cutoff' is expected to be a fraction (between 0 and 1), whereas 'trim_percent_cutoff'={}".format(
            config.input['trim_percent_cutoff'])
        assert patient_tuple not in patient_dict, "ERROR: sample '{}-{}' is not unique".format(
            row[0], row[1])

        if len(row) == 2:
            # All samples are assumed to have same read length and the default, 250 bp
            patient_dict[patient_tuple] = 250

        elif len(row) >= 3:
            # Extract read length from input.samples_file. Samples may have
            # different read lengths. Reads will be filtered out if read length
            # after trimming is less than trim_cutoff * read_length.
            patient_dict[patient_tuple] = int(row[2])

# 4. generate list of target files
all_files = []
alignments = []
vicuna_refs = []
references = []
consensus = []
trimmed_files = []
results = []
datasets = []
IDs = []
for p in patient_list:

    alignments.append(
        "{sample_dir}/{patient}/{date}/alignments/REF_aln.bam".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    if config.output['QA']:
        alignments.append(
            "{sample_dir}/{patient}/{date}/QA_alignments/coverage_ambig.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
        alignments.append(
            "{sample_dir}/{patient}/{date}/QA_alignments/coverage_majority.tsv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    vicuna_refs.append(
        "{sample_dir}/{patient}/{date}/references/vicuna_consensus.fasta".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    references.append(
        "{sample_dir}/{patient}/{date}/references/ref_".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    consensus.append(
        "{sample_dir}/{patient}/{date}/references/ref_ambig.fasta".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    consensus.append(
        "{sample_dir}/{patient}/{date}/references/ref_majority.fasta".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    trimmed_files.append(
        "{sample_dir}/{patient}/{date}/preprocessed_data/R1.fastq.gz".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    if config.input['paired']:
        trimmed_files.append("{sample_dir}/{patient}/{date}/preprocessed_data/R2.fastq.gz".format(
            sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    datasets.append("{sample_dir}/{patient}/{date}".format(
        sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    IDs.append(('{}-{}').format(p.patient_id, p.date))

    # SNV
    if config.output['snv']:
        if config.general['snv_caller'] == 'shorah':
            results.append("{sample_dir}/{patient}/{date}/variants/SNVs/snvs.csv".format(
                sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
        elif config.general['snv_caller'] == 'lofreq':
            results.append("{sample_dir}/{patient}/{date}/variants/SNVs/snvs.vcf".format(
                sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    # local haplotypes
    if config.output['local']:
        results.append(
            "{sample_dir}/{patient}/{date}/variants/SNVs/snvs.csv".format(sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
    # global haplotypes
    if config.output['global']:
        if config.general['haplotype_reconstruction'] == 'savage':
            results.append("{sample_dir}/{patient}/{date}/variants/global/contigs_stage_c.fasta".format(
                sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))
        elif config.general['haplotype_reconstruction'] == 'haploclique':
            results.append("{sample_dir}/{patient}/{date}/variants/global/quasispecies.bam".format(
                sample_dir=config.input['datadir'], patient=p.patient_id, date=p.date))

    # merge lists contaiing expected output
    all_files = alignments + consensus + results

IDs = ','.join(IDs)

# 5. Locate reference and parse reference identifier
if not VPIPE_BENCH:
    try:
        if os.path.isfile(config.input['reference']):
            reference_file = config.input['reference']
        elif os.path.isfile(os.path.join("references", config.input['reference'])):
            reference_file = os.path.join(
                "references", config.input['reference'])
    except:
        print("ERROR: Reference file {} does not exists".format(
            config.input['reference']))
        raise

    with open(reference_file, 'r') as infile:
        reference_name = infile.readline().rstrip()
    reference_name = reference_name.split('>')[1]
    reference_name = reference_name.split(' ')[0]

else:
    try:
        reference_name
    except NameError:
        reference_name = ''

functions = srcdir("functions.sh")

# DUMMY RULES
rule all:
    input:
        all_files

rule alltrimmed:
    input:
        trimmed_files


# 1. extract
rule gunzip:
    input:
        "{file}.fastq.gz"
    output:
        temp("{file}.fastq")
    params:
        scratch = '10000',
        mem = config.gunzip['mem'],
        time = config.gunzip['time'],
        GUNZIP = config.applications['gunzip'],
    log:
        outfile = temp("{file}_gunzip.out.log"),
        errfile = temp("{file}_gunzip.err.log"),
    threads:
        1
    shell:
        """
        {params.GUNZIP} -c {input} > {output}
        """


def construct_input_fastq(wildcards):
    indir = os.path.join(wildcards.dataset, "raw_data")
    aux = glob_wildcards(
        indir + "/{prefix, [^/]+}" + "{ext, (\.fastq|\.fastq\.gz|\.fq|\.fq\.gz)}")
    if config.input['paired']:
        inferred_values = glob_wildcards(
            indir + "/{file}R" + wildcards.pair + config.input['fastq_suffix'] + aux.ext[0])
    else:
        inferred_values = glob_wildcards(indir + "/{file}" + aux.ext[0])

    list_output = []
    file_extension = aux.ext[0].split(".gz")[0]
    for i in inferred_values.file:
        if config.input['paired']:
            list_output.append(indir + "/" + i + "R" + wildcards.pair +
                               config.input['fastq_suffix'] + file_extension)
        else:
            list_output.append(indir + "/" + i + file_extension)
    if len(list_output) == 0:
        print("Missing input files for rule extract: {}/raw_data/ - Unexpected file name?".format(wildcards.dataset))
        sys.exit(1)

    return list_output


rule extract:
    input:
        construct_input_fastq
    output:
        temp("{dataset}/extracted_data/R{pair}.fastq")
    params:
        scratch = '2000',
        mem = config.extract['mem'],
        time = config.extract['time'],
    log:
        outfile = "{dataset}/extracted_data/extract_R{pair}.out.log",
        errfile = "{dataset}/extracted_data/extract_R{pair}.err.log"
    benchmark:
        "{dataset}/extracted_data/extract_R{pair}.benchmark"
    threads:
        1
    shell:
        """
        cat {input} | paste - - - - | sort -k1,1 -t " " | tr "\t" "\n" > {output} 2> >(tee {log.errfile} >&2)
        """

rule extractclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/extracted_data
        """


# 2. clipping
def len_cutoff(wildcards):
    parts = wildcards.dataset.split('/')
    patient_ID = parts[1]
    date = parts[2]
    patient_tuple = patient_record(patient_id=patient_ID, date=date)
    read_len = patient_dict[patient_tuple]
    len_cutoff = int(config.input['trim_percent_cutoff'] * read_len)
    return len_cutoff


if config.input['paired']:
    rule preprocessing:
        input:
            R1 = "{dataset}/extracted_data/R1.fastq",
            R2 = "{dataset}/extracted_data/R2.fastq",
        output:
            R1gz = "{dataset}/preprocessed_data/R1.fastq.gz",
            R2gz = "{dataset}/preprocessed_data/R2.fastq.gz"
        params:
            scratch = '2000',
            mem = config.preprocessing['mem'],
            time = config.preprocessing['time'],
            LEN_CUTOFF = len_cutoff,
            PRINSEQ = config.applications['prinseq'],
        log:
            outfile = "{dataset}/preprocessed_data/prinseq.out.log",
            errfile = "{dataset}/preprocessed_data/prinseq.err.log"
        conda:
            config.preprocessing['conda']
        benchmark:
            "{dataset}/preprocessed_data/prinseq.benchmark"
        threads:
            1
        shell:
            """
            echo "The length cutoff is: {params.LEN_CUTOFF}" > {log.outfile}

            {params.PRINSEQ} -fastq {input.R1} -fastq2 {input.R2} -out_format 3 -out_good {wildcards.dataset}/preprocessed_data/R -out_bad null -ns_max_n 4 -min_qual_mean 30 -trim_qual_left 30 -trim_qual_right 30 -trim_qual_window 10 -min_len {params.LEN_CUTOFF} -log {log.outfile} 2> >(tee {log.errfile} >&2)

            # make sure that the lock held prinseq has been effectively released and propagated
            # on some networked shares this could otherwise lead to confusion or corruption
            echo "Waiting for unlocks" >&2
            for U in {wildcards.dataset}/preprocessed_data/R_{{1,2}}.fastq; do
                flock -x -o ${{U}} -c "echo ${{U}} unlocked >&2"
            done

            mv {wildcards.dataset}/preprocessed_data/R{{_,}}1.fastq
            mv {wildcards.dataset}/preprocessed_data/R{{_,}}2.fastq
            rm -f {wildcards.dataset}/preprocessed_data/R_?_singletons.fastq

            gzip {wildcards.dataset}/preprocessed_data/R1.fastq
            gzip {wildcards.dataset}/preprocessed_data/R2.fastq
            """
else:
    rule preprocessing_se:
        input:
            R1 = "{dataset}/extracted_data/R1.fastq",
        output:
            R1gz = "{dataset}/preprocessed_data/R1.fastq.gz",
        params:
            scratch = '2000',
            mem = config.preprocessing['mem'],
            time = config.preprocessing['time'],
            LEN_CUTOFF = len_cutoff,
            PRINSEQ = config.applications['prinseq'],
        log:
            outfile = "{dataset}/preprocessed_data/prinseq.out.log",
            errfile = "{dataset}/preprocessed_data/prinseq.err.log"
        conda:
            config.preprocessing['conda']
        benchmark:
            "{dataset}/preprocessed_data/prinseq.benchmark"
        threads:
            1
        shell:
            """
            echo "The length cutoff is: {params.LEN_CUTOFF}" > {log.outfile}

            {params.PRINSEQ} -fastq {input.R1} -out_format 3 -out_good {wildcards.dataset}/preprocessed_data/R -out_bad null -ns_max_n 4 -min_qual_mean 30 -trim_qual_left 30 -trim_qual_right 30 -trim_qual_window 10 -min_len {params.LEN_CUTOFF} -log {log.outfile} 2> >(tee {log.errfile} >&2)

            # make sure that the lock held prinseq has been effectively released and propagated
            # on some network shares this could otherwise lead to confusion or corruption
            echo "Waiting for unlocks" >&2
            for U in {wildcards.dataset}/preprocessed_data/R_1.fastq; do
                flock -x -o ${{U}} -c "echo ${{U}} unlocked >&2"
            done
            
            mv {wildcards.dataset}/preprocessed_data/R{{,1}}.fastq

            gzip {wildcards.dataset}/preprocessed_data/R1.fastq
            """

rule trimmingclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/preprocessed_data
        """


# 3. initial consensus sequence
rule initial_vicuna:
    input:
        global_ref = reference_file,
        R1 = "{dataset}/preprocessed_data/R1.fastq",
        R2 = lambda wildcards: wildcards.dataset + "/preprocessed_data/R2.fastq" if config.input['paired'] else [],
    output:
        "{dataset}/references/vicuna_consensus.fasta"
    params:
        scratch = '1000',
        mem = config.initial_vicuna['mem'],
        time = config.initial_vicuna['time'],
        PAIRED = 'SECOND_END_FASTQ=cleaned/R2.fastq' if config.input['paired'] else '',
        PAIRED_BOOL = 'true' if config.input['paired'] else 'false',
        VICUNA = config.applications['vicuna'],
        BWA = config.applications['bwa'],
        INDELFIXER = config.applications['indelfixer'],
        CONSENSUSFIXER = config.applications['consensusfixer'],
        PICARD = config.applications['picard'],
        SAMTOOLS = config.applications['samtools'],
        WORK_DIR = "{dataset}/initial_consensus",
        FUNCTIONS = functions,
    log:
        outfile = "{dataset}/initial_consensus/vicuna.out.log",
        errfile = "{dataset}/initial_consensus/vicuna.err.log",
    conda:
        config.initial_vicuna['conda']
    benchmark:
        "{dataset}/initial_consensus/vicuna_consensus.benchmark"
    threads:
        config.initial_vicuna['threads']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        source {params.FUNCTIONS}

        ERRFILE=$(basename {log.errfile})
        OUTFILE=$(basename {log.outfile})

        # 1. copy initial reference for bwa
        rm -rf {params.WORK_DIR}/
        mkdir -p {params.WORK_DIR}/
        cp {input.global_ref} {params.WORK_DIR}/consensus.fasta
        cd {params.WORK_DIR}

        # 2. create bwa index
        {params.BWA} index consensus.fasta 2> >(tee $ERRFILE >&2)

        # 3. create initial alignment
        if [[ {params.PAIRED_BOOL} == "true" ]]; then
            {params.BWA} mem -t {threads} consensus.fasta ../preprocessed_data/R{{1,2}}.fastq > first_aln.sam 2> >(tee -a $ERRFILE >&2)
        else
            {params.BWA} mem -t {threads} consensus.fasta ../preprocessed_data/R1.fastq > first_aln.sam 2> >(tee -a $ERRFILE >&2)
        fi
        rm consensus.fasta.*

        # 4. remove unmapped reads
        {params.SAMTOOLS} view -b -F 4 first_aln.sam > mapped.bam 2> >(tee -a $ERRFILE >&2)
        rm first_aln.sam

        # 5. extract reads
        mkdir -p cleaned
        SamToFastq {params.PICARD} I=mapped.bam FASTQ=cleaned/R1.fastq {params.PAIRED} VALIDATION_STRINGENCY=SILENT 2> >(tee -a $ERRFILE >&2)
        rm mapped.bam

        # 6. create config file
        # NOTE: Tabs are required below
        if [[ {params.PAIRED_BOOL} == "true" ]]; then
			cat > vicuna_config.txt <<- _EOF_
				minMSize	9
				maxOverhangSize	2
				Divergence	8
				max_read_overhang	2
				max_contig_overhang	10
				pFqDir	cleaned/
				batchSize	100000
				LibSizeLowerBound	100
				LibSizeUpperBound	800
				min_output_contig_len	1000
				outputDIR	./
			_EOF_
        else
			cat > vicuna_config.txt <<- _EOF_
				minMSize	9
				maxOverhangSize	2
				Divergence	8
				max_read_overhang	2
				max_contig_overhang	10
				npFqDir	cleaned/
				batchSize	100000
				min_output_contig_len	1000
				outputDIR	./
			_EOF_
        fi

        # 7. VICUNA
        OMP_NUM_THREADS={threads} {params.VICUNA} vicuna_config.txt > $OUTFILE 2> >(tee -a $ERRFILE >&2)
        rm vicuna_config.txt
        rm -r cleaned/

        # 8. fix broken header
        sed -e 's:>dg-\([[:digit:]]\+\)\s.*:>dg-\1:g' contig.fasta > contig_clean.fasta

        # 9. InDelFixer + ConsensusFixer to polish up consensus
        for i in {{1..3}}
        do
                mv consensus.fasta old_consensus.fasta
                InDelFixer {params.INDELFIXER} -i contig_clean.fasta -g old_consensus.fasta >> $OUTFILE 2> >(tee -a $ERRFILE >&2)
                sam2bam {params.SAMTOOLS} reads.sam >> $OUTFILE 2> >(tee $ERRFILE >&2)
                ConsensusFixer {params.CONSENSUSFIXER} -i reads.bam -r old_consensus.fasta -mcc 1 -mic 1 -d -pluralityN 0.01 >> $OUTFILE 2> >(tee $ERRFILE >&2)
        done

        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" consensus.fasta
        echo "" >> consensus.fasta

        # 10. finally, move into place
        mkdir -p ../references
        mv {{,../references/vicuna_}}consensus.fasta
        """

rule initial_vicuna_msa:
    input:
        vicuna_refs
    output:
        "references/initial_aln_gap_removed.fasta"
    params:
        scratch = '1250',
        mem = config.initial_vicuna_msa['mem'],
        time = config.initial_vicuna_msa['time'],
        MAFFT = config.applications['mafft'],
        REMOVE_GAPS = config.applications['remove_gaps_msa'],
    log:
        outfile = "references/MAFFT_initial_aln.out.log",
        errfile = "references/MAFFT_initial_aln.err.log",
    conda:
        config.initial_vicuna_msa['conda']
    benchmark:
        "references/MAFFT_initial_aln.benchmark"
    threads:
        config.initial_vicuna_msa['threads']
    shell:
        """
        cat {input} > initial_ALL.fasta
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} initial_ALL.fasta > references/initial_aln.fasta 2> >(tee {log.errfile} >&2)
        rm initial_ALL.fasta

        {params.REMOVE_GAPS} references/initial_aln.fasta -o {output} -p 0.5 > {log.outfile} 2> >(tee -a {log.errfile} >&2)
        """

localrules:
    create_vicuna_initial
rule create_vicuna_initial:
    input:
        "references/initial_aln_gap_removed.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    params:
        EXTRACT_SEQ = config.applications['extract_seq'],
    conda:
        config.create_vicuna_initial['conda']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        {params.EXTRACT_SEQ} {input} -o {output} -s "${{CONSENSUS_NAME}}"
        """

localrules:
    create_simple_initial
rule create_simple_initial:
    input:
        "references/cohort_consensus.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        cp {input} {output}
        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" {output}
        """

localrules:
    create_denovo_initial
rule create_denovo_initial:
    input:
        "{dataset}/references/denovo_consensus.fasta"
    output:
        "{dataset}/references/initial_consensus.fasta"
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        mkdir -p {wildcards.dataset}/references/
        cp {input} {output}
        sed -i -e "s/>.*/>${{CONSENSUS_NAME}}/" {output}
        """

rule vicunaclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/initial_consensus
        rm -rf {params.DIR}/*/*/references/vicuna_consensus.fasta
        rm -rf {params.DIR}/*/*/references/initial_consensus.fasta
        rm -rf references/initial_aln.fasta
        rm -rf references/initial_aln_gap_removed.fasta
        rm -rf references/MAFFT_initial_aln.*
        """

# change this to switch between VICUNA and creating a simple initial
# initial reference
ruleorder:
    create_denovo_initial > create_simple_initial > create_vicuna_initial
# ruleorder: create_vicuna_initial > create_simple_initial


# 4. aligning
def input_align(wildcards):
    list_output = []
    list_output.append(wildcards.dataset + "/preprocessed_data/R1.fastq")
    if config.input['paired']:
        list_output.append(wildcards.dataset + "/preprocessed_data/R2.fastq")
    return(list_output)


rule hmm_align:
    input:
        initial_ref = "{dataset}/references/initial_consensus.fasta",
        FASTQ = input_align,
    output:
        good_aln = temp("{dataset}/alignments/full_aln.sam"),
        reject_aln = temp("{dataset}/alignments/rejects.sam"),
        REF_ambig = "{dataset}/references/ref_ambig.fasta",
        REF_majority = "{dataset}/references/ref_majority.fasta",
    params:
        scratch = '1250',
        mem = config.hmm_align['mem'],
        time = config.hmm_align['time'],
        LEAVE_TEMP = '-l' if config.hmm_align['leave_msa_temp'] else '',
        MAFFT = config.applications['mafft'],
        NGSHMMALIGN = config.applications['ngshmmalign'],
    log:
        outfile = "{dataset}/alignments/ngshmmalign.out.log",
        errfile = "{dataset}/alignments/ngshmmalign.err.log",
    conda:
        config.hmm_align['conda']
    benchmark:
        "{dataset}/alignments/ngshmmalign.benchmark"
    threads:
        config.hmm_align['threads']
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        # 1. clean previous run
        rm -rf   {wildcards.dataset}/alignments
        rm -f    {wildcards.dataset}/references/ref_ambig.fasta
        rm -f    {wildcards.dataset}/references/ref_majority.fasta
        mkdir -p {wildcards.dataset}/alignments
        mkdir -p {wildcards.dataset}/references

        # 2. perform alignment # -l = leave temps
        {params.NGSHMMALIGN} -v -R {input.initial_ref} -o {output.good_aln} -w {output.reject_aln} -t {threads} -N "${{CONSENSUS_NAME}}" {params.LEAVE_TEMP} {input.FASTQ} > {log.outfile} 2> >(tee {log.errfile} >&2)

        # 3. move references into place
        mv {wildcards.dataset}/{{alignments,references}}/ref_ambig.fasta
        mv {wildcards.dataset}/{{alignments,references}}/ref_majority.fasta
        """

rule sam2bam:
    input:
        "{file}.sam"
    output:
        BAM = "{file}.bam",
        BAI = "{file}.bam.bai"
    params:
        scratch = '1250',
        mem = config.sam2bam['mem'],
        time = config.sam2bam['time'],
        SAMTOOLS = config.applications['samtools'],
        FUNCTIONS = functions,
    log:
        outfile = "{file}_sam2bam.out.log",
        errfile = "{file}_sam2bam.err.log",
    conda:
        config.sam2bam['conda']
    benchmark:
        "{file}_sam2bam.benchmark"
    threads:
        1
    shell:
        """
        # convert sam -> bam
        source {params.FUNCTIONS}
        sam2bam {params.SAMTOOLS} {input} > {log.outfile} 2> >(tee {log.errfile} >&2)
        """

# 4a. align against 5VM as a QA check
rule bwa_QA:
    input:
        patient_ref = "{dataset}/references/ref_{kind}.fasta",
        virusmix_ref = "references/5-Virus-Mix.fasta",
        FASTQ = input_align,
    output:
        SAM = temp("{dataset}/QA_alignments/bwa_QA_{kind}.sam"),
        MSA = "{dataset}/QA_alignments/bwa_refs_msa_{kind}.fasta",
    params:
        scratch = '1250',
        mem = config.bwa_QA['mem'],
        time = config.bwa_QA['time'],
        BWA = config.applications['bwa'],
        MAFFT = config.applications['mafft'],
    log:
        outfile = "{dataset}/QA_alignments/bwa_{kind}.out.log",
        errfile = "{dataset}/QA_alignments/bwa_{kind}.err.log",
    conda:
        config.bwa_QA['conda']
    benchmark:
        "{dataset}/QA_alignments/bwa_{kind}.benchmark"
    threads:
        config.bwa_QA['threads']
    shell:
        """
        # 1. cleanup old run
        rm -f {output.SAM} {output.MSA}

        # 2. concatenate references
        mkdir -p {wildcards.dataset}/QA_alignments
        cat {input.patient_ref} {input.virusmix_ref} > {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta

        # 3. indexing
        {params.BWA} index {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta 2> >(tee {log.errfile} >&2)

        # 4. align
        {params.BWA} mem -t {threads} {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta {input.FASTQ} > {output.SAM} 2> >(tee -a {log.errfile} >&2)

        # 5. MSA
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta > {output.MSA} 2> >(tee -a {log.errfile} >&2)

        # 6. cleanup BWA indices
        rm -f {wildcards.dataset}/QA_alignments/bwa_refs_{wildcards.kind}.fasta.*
        """

# 4b. Call coverage statistics
rule coverage_QA:
    input:
        BAM = "{dataset}/QA_alignments/bwa_QA_{kind}.bam",
        MSA = "{dataset}/QA_alignments/bwa_refs_msa_{kind}.fasta",
    output:
        "{dataset}/QA_alignments/coverage_{kind}.tsv",
    params:
        scratch = '1250',
        mem = config.coverage_QA['mem'],
        time = config.coverage_QA['time'],
        COV_STATS = config.applications['coverage_stats'],
    log:
        outfile = "{dataset}/QA_alignments/coverage_QA_{kind}.out.log",
        errfile = "{dataset}/QA_alignments/coverage_QA_{kind}.err.log",
    conda:
        config.coverage_QA['conda']
    benchmark:
        "{dataset}/QA_alignments/coverage_QA_{kind}.benchmark"
    threads:
        1
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        # 1. clean previous run
        rm -f {output}
        mkdir -p {wildcards.dataset}/QA_alignments

        # 2. collect coverage stats
        # we only collect statistics in the loop regions
        # of HIV-1 in order
        {params.COV_STATS} -t HXB2:6614-6812,7109-7217,7376-7478,7601-7634 -i {input.BAM} -o {output} -m {input.MSA} --select "${{CONSENSUS_NAME}}" > {log.outfile} 2> >(tee {log.errfile} >&2)
        """


# 5. construct MSA from all patient files
def construct_msa_input_files(wildcards):
    output_list = ["{}{}.fasta".format(s, wildcards.kind)
                   for s in references]
    output_list.append(reference_file)

    return output_list


rule msa:
    input:
        construct_msa_input_files
    output:
        "references/ALL_aln_{kind}.fasta"
    params:
        scratch = '1250',
        mem = config.msa['mem'],
        time = config.msa['time'],
        MAFFT = config.applications['mafft'],
    log:
        outfile = "references/MAFFT_{kind}_cohort.out.log",
        errfile = "references/MAFFT_{kind}_cohort.err.log",
    conda:
        config.msa['conda']
    benchmark:
        "references/MAFFT_{kind}_cohort.benchmark"
    threads:
        config.msa['threads']
    shell:
        """
        cat {input} > ALL_{wildcards.kind}.fasta
        {params.MAFFT} --nuc --preservecase --maxiterate 1000 --localpair --thread {threads} ALL_{wildcards.kind}.fasta > {output} 2> >(tee {log.errfile} >&2)
        rm ALL_{wildcards.kind}.fasta
        """

rule msaclean:
    shell:
        """
        rm -rf references/ALL_aln_*.fasta
        rm -rf references/MAFFT_*_cohort.*
        """


# 6. convert alignments to REF alignment
def get_reference_name(wildcards):
    with open(reference_file, 'r') as infile:
        reference_name = infile.readline().rstrip()
    reference_name = reference_name.split('>')[1]
    reference_name = reference_name.split(' ')[0]
    return reference_name


rule convert_to_ref:
    input:
        REF_ambig = "references/ALL_aln_ambig.fasta",
        REF_majority = "references/ALL_aln_majority.fasta",
        BAM = "{dataset}/alignments/full_aln.bam",
        REJECTS_BAM = "{dataset}/alignments/rejects.bam",
    output:
        "{dataset}/alignments/REF_aln.bam"
    params:
        scratch = '1250',
        mem = config.convert_to_ref['mem'],
        time = config.convert_to_ref['time'],
        REF_NAME = reference_name if reference_name else get_reference_name,
        CONVERT_REFERENCE = config.applications['convert_reference'],
    log:
        outfile = "{dataset}/alignments/convert_to_ref.out.log",
        errfile = "{dataset}/alignments/convert_to_ref.err.log",
    conda:
        config.convert_to_ref['conda']
    benchmark:
        "{dataset}/alignments/convert_to_ref.benchmark"
    threads:
        1
    shadow:
        "shallow"
    shell:
        """
        {params.CONVERT_REFERENCE} -t {params.REF_NAME} -m {input.REF_ambig} -i {input.BAM} -o {output} > {log.outfile} 2> >(tee {log.errfile} >&2)
        """


rule alignclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/alignments
        rm -rf {params.DIR}/*/*/QA_alignments
        rm -rf {params.DIR}/*/*/references/ref_ambig.fasta
        rm -rf {params.DIR}/*/*/references/ref_majority.fasta
        rm -rf {params.DIR}/*/*/references/initial_consensus.fasta
        """

# 4-6. Alternative: align reads using bwa or bowtie
if config.general["aligner"] == "bwa":
    rule ref_bwa_index:
        input:
            reference_file
        output:
            "{}.bwt".format(reference_file)
        params:
            scratch = '1250',
            mem = config.ref_bwa_index['mem'],
            time = config.ref_bwa_index['time'],
            BWA = config.applications['bwa']
        log:
            outfile = "references/bwa_index.out.log",
            errfile = "references/bwa_index.err.log",
        conda:
            config.ref_bwa_index['conda']
        benchmark:
            "references/ref_bwa_index.benchmark"
        shell:
            """
            {params.BWA} index {input} 2> >(tee {log.errfile} >&2)
            """

    rule bwa_align:
        input:
            FASTQ = input_align,
            REF = reference_file,
            INDEX = "{}.bwt".format(reference_file)
        output:
            temp("{dataset}/alignments/REF_aln.sam"),
        params:
            scratch = '1250',
            mem = config.bwa_align['mem'],
            time = config.bwa_align['time'],
            FILTER = '-f 2' if config.input['paired'] else '-F 4',
            TMP_SAM = "{dataset}/alignments/tmp_aln.sam",
            BWA = config.applications['bwa'],
            SAMTOOLS = config.applications['samtools'],
        log:
            outfile = "{dataset}/alignments/bwa_align.out.log",
            errfile = "{dataset}/alignments/bwa_align.err.log",
        conda:
            config.bwa_align['conda']
        benchmark:
            "{dataset}/alignments/bwa_align.benchmark"
        threads:
            config.bwa_align['threads']
        shell:
            """
            {params.BWA} mem -t {threads} {input.REF} {input.FASTQ} > {params.TMP_SAM} 2> >(tee {log.errfile} >&2)
            # Filter alignments: (1) remove unmapped reads (single-end) or keep only reads mapped in proper pairs (paired-end), (2) remove supplementary aligments
            {params.SAMTOOLS} view -h {params.FILTER} -F 2048 {params.TMP_SAM} > {output} 2> >(tee -a {log.errfile} >&2)
            rm {params.TMP_SAM}
            """
elif config.general["aligner"] == "bowtie":
    rule ref_bowtie_index:
        input:
            reference_file
        output:
            INDEX1 = "{}.1.bt2".format(reference_file),
            INDEX2 = "{}.2.bt2".format(reference_file),
            INDEX3 = "{}.3.bt2".format(reference_file),
            INDEX4 = "{}.4.bt2".format(reference_file),
            INDEX5 = "{}.rev.1.bt2".format(reference_file),
            INDEX6 = "{}.rev.2.bt2".format(reference_file),
        params:
            scratch = '1250',
            mem = config.ref_bowtie_index['mem'],
            time = config.ref_bowtie_index['time'],
            BOWTIE = config.applications['bowtie_idx']
        log:
            outfile = "references/bowtie_index.out.log",
            errfile = "references/bowtie_index.err.log",
        conda:
            config.ref_bowtie_index['conda']
        benchmark:
            "references/ref_bowtie_index.benchmark"
        shell:
            """
            {params.BOWTIE} {input} {input} 2> >(tee {log.errfile} >&2)
            """

    if config.input['paired']:
        rule bowtie_align:
            input:
                R1 = "{dataset}/preprocessed_data/R1.fastq.gz",
                R2 = "{dataset}/preprocessed_data/R2.fastq.gz",
                REF = reference_file,
                INDEX1 = "{}.1.bt2".format(reference_file),
                INDEX2 = "{}.2.bt2".format(reference_file),
                INDEX3 = "{}.3.bt2".format(reference_file),
                INDEX4 = "{}.4.bt2".format(reference_file),
                INDEX5 = "{}.rev.1.bt2".format(reference_file),
                INDEX6 = "{}.rev.2.bt2".format(reference_file)
            output:
                temp("{dataset}/alignments/REF_aln.sam"),
            params:
                scratch = '1250',
                mem = config.bowtie_align['mem'],
                time = config.bowtie_align['time'],
                TMP_SAM = "{dataset}/alignments/tmp_aln.sam",
                PHRED = config.bowtie_align['phred'],
                PRESET = config.bowtie_align['preset'],
                REPORT = config.bowtie_align['report'],
                BOWTIE = config.applications['bowtie'],
                SAMTOOLS = config.applications['samtools'],
            log:
                outfile = "{dataset}/alignments/bowtie_align.out.log",
                errfile = "{dataset}/alignments/bowtie_align.err.log",
            conda:
                config.bowtie_align['conda']
            benchmark:
                "{dataset}/alignments/bowtie_align.benchmark"
            threads:
                config.bowtie_align['threads']
            shell:
                """
                {params.BOWTIE} -x {input.REF} -1 {input.R1} -2 {input.R2} {params.PHRED} {params.PRESET} {params.REPORT} -p {threads} -S {params.TMP_SAM} 2> >(tee {log.errfile} >&2)
                # Filter alignments: (1) keep only reads mapped in proper pairs, and (2) remove supplementary aligments
                {params.SAMTOOLS} view -h -f 2 -F 2048 {params.TMP_SAM} > {output} 2> >(tee -a {log.errfile} >&2)
                rm {params.TMP_SAM}
                """
    else:
        rule bowtie_align_se:
            input:
                R1 = "{dataset}/preprocessed_data/R1.fastq.gz",
                REF = reference_file,
                INDEX1 = "{}.1.bt2".format(reference_file),
                INDEX2 = "{}.2.bt2".format(reference_file),
                INDEX3 = "{}.3.bt2".format(reference_file),
                INDEX4 = "{}.4.bt2".format(reference_file),
                INDEX5 = "{}.rev.1.bt2".format(reference_file),
                INDEX6 = "{}.rev.2.bt2".format(reference_file)
            output:
                temp("{dataset}/alignments/REF_aln.sam"),
            params:
                scratch = '1250',
                mem = config.bowtie_align['mem'],
                time = config.bowtie_align['time'],
                TMP_SAM = "{dataset}/alignments/tmp_aln.sam",
                PHRED = config.bowtie_align['phred'],
                PRESET = config.bowtie_align['preset'],
                REPORT = config.bowtie_align['report'],
                BOWTIE = config.applications['bowtie'],
                SAMTOOLS = config.applications['samtools'],
            log:
                outfile = "{dataset}/alignments/bowtie_align.out.log",
                errfile = "{dataset}/alignments/bowtie_align.err.log",
            conda:
                config.bowtie_align['conda']
            benchmark:
                "{dataset}/alignments/bowtie_align.benchmark"
            threads:
                config.bowtie_align['threads']
            shell:
                """
                {params.BOWTIE} -x {input.REF} -U {input.R1} {params.PHRED} {params.PRESET} -k {params.REPORT} -p {threads} -S {params.TMP_SAM} 2> >(tee {log.errfile} >&2)
                # Filter alignments: (1) remove unmapped reads, and (2) remove supplementary aligments
                {params.SAMTOOLS} view -h -F 4 -F 2048 {params.TMP_SAM} > {output} 2> >(tee -a {log.errfile} >&2)
                rm {params.TMP_SAM}
                """

rule bwaclean:
    input:
        "{}.bwt".format(reference_file)
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -f {input}
        rm -rf {params.DIR}/*/*/alignments
        """

rule bowtieclean:
    input:
        INDEX1 = "{}.1.bt2".format(reference_file),
        INDEX2 = "{}.2.bt2".format(reference_file),
        INDEX3 = "{}.3.bt2".format(reference_file),
        INDEX4 = "{}.4.bt2".format(reference_file),
        INDEX5 = "{}.rev.1.bt2".format(reference_file),
        INDEX6 = "{}.rev.2.bt2".format(reference_file)
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -f {input}
        rm -rf {params.DIR}/*/*/alignments
        """


rule consensus_sequences:
    input:
        BAM = "{dataset}/alignments/REF_aln.bam",
        REF = reference_file,
    output:
        REF_amb = "{dataset}/references/ref_ambig.fasta",
        REF_majority = "{dataset}/references/ref_majority.fasta",
    params:
        scratch = '1250',
        mem = config.consensus_sequences['mem'],
        time = config.consensus_sequences['time'],
        MIN_COVERAGE = config.consensus_sequences['min_coverage'],
        QUAL_THRD = config.consensus_sequences['qual_thrd'],
        MIN_FREQ = config.consensus_sequences['min_freq'],
        OUTDIR = "{dataset}/references",
        EXTRACT_CONSENSUS = config.applications['extract_consensus'],
    log:
        outfile = "{dataset}/references/consensus_sequences.out.log",
        errfile = "{dataset}/references/consensus_sequences.err.log",
    conda:
        config.consensus_sequences['conda']
    benchmark:
        "{dataset}/alignments/consensus.benchmark"
    threads:
        1
    shell:
        """
        CONSENSUS_NAME={wildcards.dataset}
        CONSENSUS_NAME="${{CONSENSUS_NAME#*/}}"
        CONSENSUS_NAME="${{CONSENSUS_NAME//\//-}}"

        # NOTE: function below not yet part of the release submitted to bioconda
        {params.EXTRACT_CONSENSUS} -i {input.BAM} -f {input.REF} -c {params.MIN_COVERAGE} -q {params.QUAL_THRD} -a {params.MIN_FREQ} -N "${{CONSENSUS_NAME}}" -o {params.OUTDIR}
        """


if config.general["aligner"] == "ngshmmalign":
    ruleorder: hmm_align > consensus_sequences
    ruleorder: convert_to_ref > sam2bam
elif config.general["aligner"] == "bwa":
    ruleorder: consensus_sequences > hmm_align
    ruleorder: sam2bam > convert_to_ref
elif config.general["aligner"] == "bowtie":
    ruleorder: consensus_sequences > hmm_align
    ruleorder: sam2bam > convert_to_ref

# 7. Ouptut minor allele frequencies
rule minor_variants:
    input:
        REF = reference_file,
        BAM = expand("{dataset}/alignments/REF_aln.bam", dataset=datasets),
    output:
        VARIANTS = "variants/minority_variants.tsv",
        CONSENSUS = "variants/cohort_consensus.fasta",
        COVERAGE = "variants/coverage.tsv"
    params:
        scratch = '1250',
        mem = config.minor_variants['mem'],
        time = config.minor_variants['time'],
        OUTDIR = "variants",
        NAMES = IDs,
        MINORITY_CALLER = config.applications['minority_freq'],
    log:
        outfile = "variants/minority_variants.out.log",
        errfile = "variants/minority_variants.out.log",
    conda:
        config.minor_variants['conda']
    benchmark:
        "variants/minority_variants.benchmark"
    threads:
        config.minor_variants['threads']
    shell:
        """
        {params.MINORITY_CALLER} -r {input.REF} -N {params.NAMES} -t {threads} -o {params.OUTDIR} -d {input.BAM} > >(tee {log.outfile}) 2>&1
        """

# 8. Call single nucleotide variants


def window_lengths(wildcards):
    window_len = []
    for p in patient_list:
        read_len = patient_dict[p]
        aux = int(
            (read_len * 4 / 5 + config.snv['shift']) / config.snv['shift'])
        window_len.append(str(aux * config.snv['shift']))

    window_len = ','.join(window_len)
    return window_len


def shifts(wildcards):
    shifts = []
    for p in patient_list:
        read_len = patient_dict[p]
        aux = int(
            (read_len * 4 / 5 + config.snv['shift']) / config.snv['shift'])
        shifts.append(str(aux))

    shifts = ','.join(shifts)
    return shifts


rule coverage_intervals:
    input:
        BAM = expand("{dataset}/alignments/REF_aln.bam", dataset=datasets),
        TSV = "variants/coverage.tsv",
    output:
        "variants/coverage_intervals.tsv"
    params:
        scratch = '1250',
        mem = config.coverage_intervals['mem'],
        time = config.coverage_intervals['time'],
        WINDOW_LEN = window_lengths,
        COVERAGE = config.coverage_intervals['coverage'],
        OVERLAP = '' if config.coverage_intervals['overlap'] else '-cf variants/coverage.tsv',
        SHIFT = shifts,
        NAMES = IDs,
        LIBERAL = '-e' if config.coverage_intervals['liberal'] else '',
        EXTRACT_COVERAGE_INTERVALS = config.applications['extract_coverage_intervals']
    log:
        outfile = "variants/coverage_intervals.out.log",
        errfile = "variants/coverage_intervals.out.log",
    conda:
        config.coverage_intervals['conda']
    benchmark:
        "variants/coverage_intervals.benchmark"
    threads:
        config.coverage_intervals['threads']
    shell:
        """
        {params.EXTRACT_COVERAGE_INTERVALS} -c {params.COVERAGE} -w {params.WINDOW_LEN} -s {params.SHIFT} -N {params.NAMES} {params.LIBERAL} {params.OVERLAP} -t {threads} -o {output} {input.BAM} > >(tee {log.outfile}) 2>&1
        """

localrules:
    shorah_regions
rule shorah_regions:
    input:
        "variants/coverage_intervals.tsv"
    output:
        temp(
            expand("{dataset}/variants/coverage_intervals.tsv", dataset=datasets))
    params:
        scratch = '1250',
    threads:
        1
    run:
        with open(input[0], 'r') as infile:
            for line in infile:
                parts = line.rstrip().split('\t')
                patientID = parts[0].split('-')
                sample_date = patientID[-1]
                patientID = '-'.join(patientID[:-1])
                if len(parts) == 2:
                    regions = parts[1].split(',')
                else:
                    regions = []

                with open(os.path.join(config.input['datadir'], patientID, sample_date, "variants", "coverage_intervals.tsv"), 'w') as outfile:
                    outfile.write('\n'.join(regions))


def read_len(wildcards):
    parts = wildcards.dataset.split('/')
    patient_ID = parts[1]
    date = parts[2]
    patient_tuple = patient_record(patient_id=patient_ID, date=date)
    read_len = patient_dict[patient_tuple]
    return read_len


rule snv:
    input:
        REF = "variants/cohort_consensus.fasta",
        BAM = "{dataset}/alignments/REF_aln.bam",
        TSV = "{dataset}/variants/coverage_intervals.tsv",
    output:
        "{dataset}/variants/SNVs/snvs.csv"
    params:
        scratch = '1250',
        mem = config.snv['mem'],
        time = config.snv['time'],
        READ_LEN = read_len,
        SHIFT = config.snv['shift'],
        KEEP_FILES = 'true' if config.snv['keep_files'] else 'false',
        WORK_DIR = "{dataset}/variants/SNVs",
        SHORAH = config.applications['shorah']
    log:
        outfile = "{dataset}/variants/SNVs/shorah.out.log",
        errfile = "{dataset}/variants/SNVs/shorah.err.log",
    conda:
        config.snv['conda']
    benchmark:
        "{dataset}/variants/SNVs/shorah.benchmark"
    threads:
        config.snv['threads']
    shell:
        """
        let "WINDOW_SHIFTS=({params.READ_LEN} * 4/5 + {params.SHIFT}) / {params.SHIFT}"
        let "WINDOW_LEN=WINDOW_SHIFTS * {params.SHIFT}"

        echo "Windows are shifted by: ${{WINDOW_SHIFTS}} bp" > {log.outfile}
        echo "The window length is: ${{WINDOW_LEN}} bp" >> {log.outfile}

        # Get absolute path for input files
        CWD=${{PWD}}
        BAM=${{PWD}}/{input.BAM}
        REF=${{PWD}}/{input.REF}
        OUTFILE=${{PWD}}/{log.outfile}
        ERRFILE=${{PWD}}/{log.errfile}
        WORK_DIR=${{PWD}}/{params.WORK_DIR}

        # Run ShoRAH in each of the predetermined regions (regions with sufficient coverage)
        LINE_COUNTER=0
        FILES=""
        while read -r region || [[ -n ${{region}} ]]
        do
            echo "Running ShoRAH on region: ${{region}}" >> $OUTFILE
            LINE_COUNTER=$(( $LINE_COUNTER + 1))
            # Create directory for running ShoRAH in a corresponding region (if doesn't exist)
            DIR=${{WORK_DIR}}/REGION_${{LINE_COUNTER}}
            if [[ ! -d "${{DIR}}" ]]; then
                echo "Creating directory ${{DIR}}" >> $OUTFILE
                mkdir -p ${{DIR}}
            else
                # Results from previous runs
                if [[ {params.KEEP_FILES} == "true" ]]; then
                    DIR_DST=${{WORK_DIR}}/old
                    echo "Moving results from a previous run to ${{DIR_DST}}" >> $OUTFILE
                    rm -rf ${{DIR_DST}}/REGION_${{LINE_COUNTER}}
                    mkdir -p ${{DIR_DST}}
                    mv -f ${{DIR}} ${{DIR_DST}}
                    mkdir -p ${{DIR}}
                fi
            fi
            # Change to the directory where ShoRAH is to be executed
            cd ${{DIR}}

            # NOTE: Execution command for ShoRAH2, not yet in bioconda
            {params.SHORAH} -w ${{WINDOW_LEN}} -x 100000 -r ${{region}} -R 42 -b ${{BAM}} -f ${{REF}} >> $OUTFILE 2> >(tee -a $ERRFILE >&2)
            if [[ -f ${{DIR}}/snv/SNVs_0.010000_final.csv ]]; then
                FILES="$FILES ${{DIR}}/snv/SNVs_0.010000_final.csv"
            else
                echo "ERROR: unsuccesful execution of ShoRAH" 2> >(tee -a $ERRFILE >&2)
                exit 1
            fi

            # Change back to working directory
            cd ${{CWD}}
        done < {input.TSV}

        # Aggregate results from different regions
        if [[ -z ${{FILES}} ]]; then
            if [[ ${{LINE_COUNTER}} > 0 ]]; then
                echo "ERROR: unsuccesful execution of ShoRAH" 2> >(tee -a {log.errfile} >&2)
                exit 1
            else
                echo "No alignment region reports sufficient coverage" >> {log.outfile}
                touch {output}
            fi
        else
            echo "Intermediate files: ${{FILES}}" >> {log.outfile}
            cat ${{FILES}} | sort -t, -nk2 | tail -n +${{LINE_COUNTER}} > {output}
        fi
        """

rule snvclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/variants/SNVs
        """

rule lofreq:
    input:
        REF = "variants/cohort_consensus.fasta",
        BAM = "{dataset}/alignments/REF_aln.bam",
        TSV = "{dataset}/variants/coverage_intervals.tsv",
    output:
        BAM = "{dataset}/variants/SNVs/REF_aln_indelqual.bam",
        SNVs = "{dataset}/variants/SNVs/snvs.vcf"
    params:
        scratch = '2000',
        mem = config.lofreq['mem'],
        time = config.lofreq['time'],
        OUTDIR = "{dataset}/variants/SNVs",
        SAMTOOLS = config.applications['samtools'],
        BCFTOOLS = config.applications['bcftools'],
        LOFREQ = config.applications['lofreq'],
    log:
        outfile = "{dataset}/variants/SNVs/lofreq.out.log",
        errfile = "{dataset}/variants/SNVs/lofreq.out.log",
    conda:
        config.lofreq['conda']
    shell:
        """
        # Add qualities to indels
        {params.LOFREQ} indelqual --dindel -f {input.REF} -o {output.BAM} --verbose {input.BAM} > >(tee -a {log.outfile}) 2>&1
        # Index bam file
        {params.SAMTOOLS} index {output.BAM}

        # Run Lofreq
        # NOTE: lofreq reads the region as a closed interval and uses 1-based indexing
        LINE_COUNTER=0
        FILES=""
        while read -r region || [[ -n ${{region}} ]]
        do
            LINE_COUNTER=$(( $LINE_COUNTER + 1 ))
            echo "Running Lofreq in region: ${{region}}" >> {log.outfile}
            OUTFILE_REGION={params.OUTDIR}/snvs_${{LINE_COUNTER}}.vcf
            {params.LOFREQ} call --call-indels -f {input.REF} -r ${{region}} -o ${{OUTFILE_REGION}} --verbose {output.BAM} > >(tee -a {log.outfile}) 2>&1
            {params.BCFTOOLS} view ${{OUTFILE_REGION}} -Oz -o ${{OUTFILE_REGION}}.gz
            {params.BCFTOOLS} index ${{OUTFILE_REGION}}.gz
            FILES="$FILES ${{OUTFILE_REGION}}.gz"
        done < {input.TSV}

        # Aggregate results from different regions
        if [[ ((${{LINE_COUNTER}} > 1)) ]]; then
            {params.BCFTOOLS} merge -o {output.SNVs} ${{FILES}}
        else
            mv ${{OUTFILE_REGION}} {output.SNVs}
        fi
        """

rule haploclique:
    input:
        "{dataset}/alignments/REF_aln.bam"
    output:
        FASTA = "{dataset}/variants/global/quasispecies.fasta",
        BAM = "{dataset}/variants/global/quasispecies.bam",
    params:
        scratch = '1250',
        mem = config.haploclique['mem'],
        time = config.haploclique['time'],
        RELAX = '--edge_quasi_cutoff_cliques=0.85 --edge_quasi_cutoff_mixed=0.85 --edge_quasi_cutoff_single=0.8 --min_overlap_cliques=0.6 --min_overlap_single=0.5' if config.haploclique[
            'relax'] else '',
        NO_SINGLETONS = '--no_singletons' if config.haploclique['no_singletons'] else '',
        NO_PROB0 = '--no_prob0' if config.haploclique['no_prob0'] else '',
        CLIQUE_SIZE_LIMIT = config.haploclique['clique_size_limit'],
        MAX_NUM_CLIQUES = config.haploclique['max_num_cliques'],
        OUTPREFIX = "{dataset}/variants/global/quasispecies",
        HAPLOCLIQUE = config.applications['haploclique'],
    log:
        outfile = "{dataset}/variants/global/haploclique.out.log",
        errfile = "{dataset}/variants/global/haploclique.err.log",
    conda:
        config.haploclique['conda']
    benchmark:
        "{dataset}/variants/global/haploclique.benchmark"
    threads:
        1
    shell:
        """
        {params.HAPLOCLIQUE} {params.RELAX} {params.NO_SINGLETONS} {params.NO_PROB0} --limit_clique_size={params.CLIQUE_SIZE_LIMIT} --max_cliques={params.MAX_NUM_CLIQUES} --log={log.outfile} --bam {input} {params.OUTPREFIX} 2> >(tee {log.errfile} >&2)
        """

rule haploclique_visualization:
    input:
        BAM = "{dataset}/variants/global/quasispecies.bam",
        FASTA = "{dataset}/variants/global/quasispecies.fasta",
    output:
        PDF = "{dataset}/variants/global/quasispecies_plot.pdf",
    params:
        scratch = '1250',
        mem = config.haploclique_visualization['mem'],
        time = config.haploclique_visualization['time'],
        REGION_START = config.haploclique_visualization['region_start'],
        REGION_END = config.haploclique_visualization['region_end'],
        USE_MSA = '-r' if len(
            config.haploclique_visualization['msa']) > 0 else '',
        MSA = config.haploclique_visualization['msa'],
        TSV = "{dataset}/variants/global/quasispecies_mapping.tsv",
        INPREFIX = "{dataset}/variants/global/quasispecies",
        COMPUTE_MDS = config.applications['compute_mds'],
    log:
        outfile = "{dataset}/variants/global/haploclique_visualization.out.log",
        errfile = "{dataset}/variants/global/haploclique_visualization.err.log",
    conda:
        config.haploclique_visualization['conda']
    benchmark:
        "{dataset}/variants/global/haploclique_visualization.benchmark"
    threads:
        1
    shell:
        """
        {params.COMPUTE_MDS} -q {params.INPREFIX} -s {params.REGION_START} -e {params.REGION_END} {params.USE_MSA} {params.MSA} -p {output.PDF} -o {params.TSV} > {log.output} 2> >(tee {log.errfile} >&2)
        """

rule haplocliqueclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm {params.DIR}/*/*/variants/global/quasispecies.*
        """

if config.input['paired']:
    rule savage:
        input:
            "{dataset}/alignments/REF_aln.bam"
        output:
            R1 = temp("{dataset}/variants/global/R1.fastq"),
            R2 = temp("{dataset}/variants/global/R2.fastq"),
            FASTA = "{dataset}/variants/global/contigs_stage_c.fasta",
        params:
            scratch = '1250',
            mem = config.savage['mem'],
            time = config.savage['time'],
            SPLIT = config.savage['split'],
            PICARD = config.applications['picard'],
            SAVAGE = config.applications['savage'],
            OUTDIR = "{dataset}/variants/global/",
            FUNCTIONS = functions,
        log:
            outfile = "{dataset}/variants/global/savage.out.log",
            errfile = "{dataset}/variants/global/savage.err.log",
        conda:
            config.savage['conda']
        threads:
            config.savage['threads']
        shell:
            """
            # Convert BAM to FASTQ without re-reversing reads - SAVAGE expect all reads in the same direction
            source {params.FUNCTIONS}
            SamToFastq {params.PICARD} I={input} FASTQ={output.R1} SECOND_END_FASTQ={output.R2} RC=false 2> >(tee -a {log.errfile} >&2)
            # Remove /1 and /2 from the read names
            sed -i -e "s:/1$::" {output.R1}
            sed -i -e "s:/2$::" {output.R2}

            R1=${{PWD}}/{output.R1}
            R2=${{PWD}}/{output.R2}
            {params.SAVAGE} -t {threads} --split {params.SPLIT} -p1 ${{R1}} -p2 ${{R2}} -o {params.OUTDIR}
            """
else:
    rule savage_se:
        input:
            "{dataset}/alignments/REF_aln.bam"
        output:
            R1 = temp("{dataset}/variants/global/R1.fastq"),
            FASTA = "{dataset}/variants/global/contigs_stage_c.fasta",
        params:
            scratch = '1250',
            mem = config.savage['mem'],
            time = config.savage['time'],
            SPLIT = config.savage['split'],
            PICARD = config.applications['picard'],
            SAVAGE = config.applications['savage'],
            OUTDIR = "{dataset}/variants/global/",
            FUNCTIONS = functions,
        log:
            outfile = "{dataset}/variants/global/savage.out.log",
            errfile = "{dataset}/variants/global/savage.err.log",
        conda:
            config.savage['conda']
        threads:
            config.savage['threads']
        shell:
            """
            # Convert BAM to FASTQ without re-reversing reads - SAVAGE expect all reads in the same direction
            source {params.FUNCTIONS}
            SamToFastq {params.PICARD} I={input} FASTQ={output.R1} 2> >(tee -a {log.errfile} >&2)

            R1=${{PWD}}/{output.R1}
            {params.SAVAGE} -t {threads} --split {params.SPLIT} -s ${{R1}} -o {params.OUTDIR}
            """

rule savageclean:
    params:
        DIR = config.input['datadir']
    shell:
        """
        rm -rf {params.DIR}/*/*/variants/global/contigs_stage_?.fasta
        rm -rf {params.DIR}/*/*/variants/global/stage_?
        """
