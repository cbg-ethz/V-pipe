"""
Input to the workflow is the current date.
Output is a file with the mutations of interest and heatmap

1. Get sampels of interests --> here Augustes code is probably useful
2. use bcf file generated by V-pipe
3. Translate bcf file to vcf
4. Annoated vcf file with amino acids
5. filter and merged togehter annoated files to csv
6. Collect interesting coverage information for posiitons of interests
7. Generate heatmap
"""
import yaml
from datetime import date, datetime, timedelta
import pandas as pd

def load_config(path):
    with open(path, 'r') as stream:
        config = yaml.safe_load(stream)
    return config

config = load_config("config/config.yaml")

# rule definitions
rule all:
    input:
        "results/signatures/mutations_of_interest.csv",
        "results/signatures/coverage.mutations_of_interest.csv",
        "results/signatures/heatmap.mutations_of_interest.csv",

## 1. Get sampels of interests
fpath_timeline = config['fpath_timeline']
enddatetime =  datetime.now()
startdatetime = enddatetime - timedelta(weeks=3)

# correct format
startdatetime = datetime.strptime(str(startdatetime.strftime('%Y-%m-%d')), '%Y-%m-%d')
enddatetime = datetime.strptime(str(enddatetime.strftime('%Y-%m-%d')), '%Y-%m-%d')

## This function is copied from https://github.com/gordonkoehn/UsefulGnom/blob/main/src/usefulgnom/serialize/coverage.py#L8
## and was created by Auguste and Gordon
def extract_sample_ID(
    timeline_file_dir: str,
    startdate: datetime = datetime.strptime("2024-01-01", "%Y-%m-%d"),
    enddate: datetime = datetime.strptime("2024-07-03", "%Y-%m-%d"),
) -> pd.DataFrame:
    """
    This funtion is copied from https://github.com/gordonkoehn/UsefulGnom/blob/main/
    Extract the sample ID of the samples from selected time period,
    location, and protocol. extract the date of the samples.

    Seelects sampled from 2022-07 to 2023-03 and location ZÃ¼rich by default.

    Args:
        timeline_file_dir (str): Path to the timeline file.
        startdate (datetime): Start date of the time period.
        enddate (datetime): End date of the time period.
        location (str): Location of the samples.
        protocol (str): Sequencing protocol used.
                         eg. for filtering condition to take
                             only Artic v4.1 protocol: "v41"

    Returns:
        pd.DataFrame: DataFrame containing the sample ID and date.
    """
    timeline_file = pd.read_csv(
        timeline_file_dir,
        sep="\t",
        usecols=["sample", "batch", "proto", "date", "location"],
        encoding="utf-8",
    )
    # convert the "date" column to datetime type:
    timeline_file["date"] = pd.to_datetime(timeline_file["date"])

    selected_rows = timeline_file[
        # TODO: remove subsample selection line below
        # (according to samples.wastewateronly.ready.tsv)
        (timeline_file["date"] > startdate.strftime("%Y-%m-%d"))
        & (timeline_file["date"] < enddate.strftime("%Y-%m-%d"))
    ]
    selected_rows['sample+batch'] = selected_rows["sample"] + "/" + selected_rows["batch"]
    samples_ID = selected_rows['sample+batch'].unique()
    return list(samples_ID)

# get samples_IDs from the specified location, time and sequencing protocol
all_samples = extract_sample_ID(
        fpath_timeline, startdatetime, enddatetime
    )

# vcf file is broken for this sample
# [E::bcf_write] Broken VCF record, the number of columns at NC_045512.2:27807 does not match the number of samples (0 vs 1)
all_samples = [x for x in all_samples if x !="G2_15_2024_12_01/20241213_2414430045"]

## 2. use bcf file generated by V-pipe
## 3. Translate bcf file to vcf
rule bcf2vcf:
    input:
        fname_bcf=config['base_path_vpipe_output']+"{sample}/references/consensus.bcftools.bcf.gz",
    output:
        fname_vcf=config['base_path_vpipe_output']+"{sample}/references/consensus.bcftools.vcf",
    conda:
        "envs/annotate_vcf.yaml"
    shell:
        """
        bcftools view -O v -o {output.fname_vcf} {input.fname_bcf}
        bcftools norm -m -both -o {output.fname_vcf} {output.fname_vcf}
        """


## 4. Annoated vcf file with amino acids
rule annotate_vcf:
    input:
        fname_snvs_vcf=config['base_path_vpipe_output']+"{sample}/references/consensus.bcftools.vcf",
        fname_genbank_file=config['fpath_genome_annotation'],
    output:
        fname_snvs_vcf=config['base_path_vpipe_output']+"{sample}/references/consensus.bcftools.annotated.vcf",
    conda:
        "envs/annotate_vcf.yaml"
    script:
        "./scripts/annotate_vcf.py"


## 5. filter and merged togehter annoated files to csv
rule filter_mutations_of_interest:
    input:
        fname_snvs_vcf=config['base_path_vpipe_output']+"{sample}/references/consensus.bcftools.annotated.vcf",
    output:
        fname_snvs_csv=config['base_path_vpipe_output']+"{sample}/references/consensus.bcftools.annotated.mutations_of_interest.csv",
    params:
        fname_mutation_list=config['fpath_mutation_of_interests'],
        fname_timeline=config['fpath_timeline'],
    conda:
        "envs/annotate_vcf.yaml"
    script:
        "./scripts/filter_mutations.py"


## 5. filter and merged togehter annoated files to csv
rule merge_mutations_of_interest:
    input:
        fnames_snv_csv=[config['base_path_vpipe_output']+f"{sample}/references/consensus.bcftools.annotated.mutations_of_interest.csv"
        for sample in all_samples],
    output:
        fname_result_csv_filtered="results/signatures/mutations_of_interest.csv",
    run:
        import pandas as pd
        pd.concat([pd.read_csv(f_snv_csv) for f_snv_csv in input.fnames_snv_csv]).to_csv(output.fname_result_csv_filtered)



## 6. Collect interesting coverage information for posiitons of interests
rule filter_coverage:
    input:
        fname=config['base_path_vpipe_output']+"{sample}/alignments/coverage.tsv.gz" , # 1-based
        fname_mutation_list=config['fpath_mutation_of_interests'],
    params:
        fname_mutation_list=config['fpath_mutation_of_interests'],
    output:
        fname_csv=config['base_path_vpipe_output']+"{sample}/alignments/coverage.mutations_of_interest.csv",
    run:
        import pandas as pd
        # load DRM
        nucleotide_positions_muts_list = pd.read_csv(params.fname_mutation_list)['PosNucleotide'].unique()
        df =  pd.read_csv(input.fname, sep="\t")
        column_coverage = df.columns[-1]
        df = df.rename(columns={column_coverage: "coverage"})
        df['sample'] = column_coverage
        df[df['pos'].isin(nucleotide_positions_muts_list)].to_csv(output.fname_csv)

rule merge_coverage:
    input:
        fnames_snv_csv=[config['base_path_vpipe_output']+f"{sample}/alignments/coverage.mutations_of_interest.csv"
        for sample in all_samples],
    output:
        fname_result_csv_filtered="results/signatures/coverage.mutations_of_interest.csv",
    run:
        import pandas as pd
        pd.concat([pd.read_csv(f_snv_csv) for f_snv_csv in input.fnames_snv_csv]).to_csv(output.fname_result_csv_filtered)


## 7. Prepare heatmap dataframe
rule prepare_heatmap:
    input:
        fname_muts="results/signatures/mutations_of_interest.csv",
        fname_cov="results/signatures/coverage.mutations_of_interest.csv",
    output:
        fname_heatmap="results/signatures/heatmap.mutations_of_interest.csv",
    run:
        import pandas as pd
        import numpy as np

        df_muts = pd.read_csv(input.fname_muts)
        df_muts['mutation_id'] = df_muts['gene']+":"+df_muts['Mutation']
        df_muts['Frequency'] =  df_muts['n_AltReads']/(df_muts['n_RefReads']+df_muts['n_AltReads'])

        df_cov = pd.read_csv(input.fname_cov)

        orders = df_muts.sort_values('POS')['mutation_id'].unique()

        data = df_muts.pivot_table(index=['date', "location", "sample"],
                                values='Frequency',
                                columns=['mutation_id', 'POS'],
                                fill_value=0,
                                dropna=False,
                               ).reset_index()

        data = pd.melt(data,
                       id_vars=['date', "location", "sample"],
                       var_name=['mutation_id', 'POS'],
                       value_name='Frequency')

        data = pd.merge(data, df_cov[['pos', 'sample', 'coverage']],
                        left_on = ['sample', 'POS'],
                        right_on= ['sample', 'pos'])

        data.loc[data['coverage']<1, 'Frequency'] = np.nan

        data = data.pivot_table(index=[ 'date', 'location'],
                                values='Frequency',
                                columns='mutation_id',
                               ).reindex(columns=orders)
        data.to_csv(output.fname_heatmap)


## 8. To get the heatmap run jupyter notebook: workflow/notebooks/heatmap.mutations_of_interest.ipynb
